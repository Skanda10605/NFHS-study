{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3c575e-3509-41fe-8c25-ac0e8a107ce9",
   "metadata": {},
   "source": [
    "Prof joshi wanted me to see how closely the features are correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a22df-48c6-4e3b-9490-a438f62009c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "pdf = pd.read_stata(\"IAIR7EFL.DTA\",convert_categoricals = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf180e9-f9f3-402a-af4c-aaad0db573ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.read_csv('Table on 27 April.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df37042-c295-4817-819f-9ae46ef26ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['normal waist circumference'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff4acb-55b5-44d0-8d10-3cbfd814488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['normal waist circumference'] += ((pdf['s305'] < 80) & (pdf['s305'] > 50)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8015b-feee-48e3-96f0-f6a17649ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['normal waist and bmi'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e877252-1fb5-4406-b144-a036b0098fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['normal waist and bmi'] += ((table['normal bmi'] == 1) & (table['normal waist circumference'] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a739b41a-0864-4b70-8a2a-4b6212f04dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5759ecf-2035-4b3f-bfaf-f103b5918c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal waist circumference'] = 0\n",
    "table['abnormal waist or bmi'] = 0\n",
    "table['abnormal bp'] = 0\n",
    "table['abnormal glucose'] = 0\n",
    "table['abnormal haemoglobin'] = 0\n",
    "table['abnormal bmi'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ca3eed-d94a-4eb8-afcf-0b8caf54e393",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal bmi'] = 1 - table['normal bmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200b0b7-3032-40c4-bfb9-f41caa4d2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal bp'] = 1 - table['normal bp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b824b9b-bc8c-469a-bde3-2fedf40cf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal haemoglobin'] = 1 - table['Normal haemoglobin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d562227-98fb-4015-bd6c-0ad9a8b02132",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal waist or bmi'] = 1 - table['normal waist and bmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c621e-158f-47ee-9239-43a72f4aa860",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal glucose'] = 1 - table['normal glucose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f8430-fe34-4b56-b4f4-0fd77176c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "table['abnormal waist circumference'] = 1 - table['normal waist circumference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9ff655-0222-4c17-9b79-164453bff6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be61643-6868-4c08-88ac-e3e1dd0cac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1 = table.iloc[:,1:6].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca6c066-97ae-4c6d-8e7e-9cb6b2cdb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = table.iloc[:,7:]\n",
    "tab.drop('abnormal waist or bmi', axis = 1, inplace = True)\n",
    "corr2 = tab.iloc[:,:].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7175ee92-9b79-42d3-9fde-26d985cf86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(corr):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, linewidths=0.5)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680729b-469d-4d71-988d-4aaaa32e68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(corr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f6a8d-b994-428f-97fa-c955b7073f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(corr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1538cb9-185f-4b9c-b622-fdd5b7b65d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table[table['abnormal waist or bmi'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e347a9-a26d-4745-860a-d58783f2e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "table[table['abnormal bmi'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece8575-0b8e-4d33-b3d4-808f9fecddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33801f6-dd23-46e9-90ce-0924d8291c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from upsetplot import UpSet, from_indicators\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure columns are boolean\n",
    "table['abnormal waist circumference'] = table['abnormal waist circumference'].astype(bool)\n",
    "table['abnormal bmi'] = table['abnormal bmi'].astype(bool)\n",
    "\n",
    "# Create UpSet data\n",
    "upset_data = from_indicators(['abnormal waist circumference', 'abnormal bmi'], table)\n",
    "\n",
    "# Increase figure size (e.g., 10 inches wide and 6 inches tall)\n",
    "plt.figure(figsize=(25, 25))\n",
    "\n",
    "# Plot\n",
    "upset = UpSet(upset_data, show_counts=True)\n",
    "upset.plot()\n",
    "\n",
    "plt.suptitle(\"UpSet Plot of Abnormal Waist Circumference and BMI\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea50f6b-9d69-4ee2-83b7-f8991b65a76b",
   "metadata": {},
   "source": [
    "In the next few sections, i was trying to cluster the individuals badsed on the metrics: 'abnormal waist circumference', 'abnormal bp', 'abnormal glucose', 'abnormal haemoglobin' and 'abnormal bmi'. I took the help of chatgpt here as my code was taking forever to run.\n",
    "\n",
    "I realised i was not doing the right thing cause the goal right now was to cluster the featured and not individuals.\n",
    "\n",
    "So, in the last section (hierarchical clusering), i clustered the features and not individuals.\n",
    "\n",
    "I have also written down how each alogirthm works for our understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965e09f-8dfe-48cf-ab67-ac519598d2eb",
   "metadata": {},
   "source": [
    "MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6d29b-0753-467a-bef0-b3efaa6a197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Step 1: Select features and drop missing values\n",
    "features = [\n",
    "    'abnormal waist circumference', 'abnormal bp', 'abnormal glucose',\n",
    "    'abnormal haemoglobin', 'abnormal bmi'\n",
    "]\n",
    "\n",
    "df = table[features].dropna()\n",
    "\n",
    "# Step 2: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Step 3: Run MiniBatchKMeans\n",
    "k = 3  # You can experiment with other values too\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=k,\n",
    "    batch_size=10200000,        # Adjust based on RAM; 10k is a safe starting point\n",
    "    n_init='auto',           # Faster, modern initialization (>= scikit-learn 1.2)\n",
    "    random_state=42\n",
    ")\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Step 4: Add cluster labels to DataFrame\n",
    "df['Cluster'] = cluster_labels\n",
    "\n",
    "# Optional: Save back to original `table`\n",
    "table.loc[df.index, 'Cluster'] = df['Cluster']\n",
    "\n",
    "# Step 5: Quick cluster summary\n",
    "print(df['Cluster'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f0db8-353c-40dd-9d68-7b6649ec100b",
   "metadata": {},
   "source": [
    "Visualising cluster (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95729234-e236-4ebc-be5e-2b32000474f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' has scaled health features + 'Cluster' from MiniBatchKMeans\n",
    "features = [\n",
    "    'abnormal waist circumference', 'abnormal bp', 'abnormal glucose',\n",
    "    'abnormal haemoglobin', 'abnormal bmi'\n",
    "]\n",
    "\n",
    "# Step 1: Compute cluster-wise means\n",
    "cluster_summary = df.groupby('Cluster')[features].mean()\n",
    "\n",
    "# Step 2: Heatmap of cluster-wise feature averages\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.heatmap(cluster_summary, annot=True, fmt=\".2f\", cmap='YlGnBu', cbar_kws={'label': 'Mean Value'})\n",
    "plt.title(\"Cluster-wise Feature Averages\")\n",
    "plt.xlabel(\"Health Features\")\n",
    "plt.ylabel(\"Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Boxplots per feature by cluster\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=df, x='Cluster', y=feature, palette='Set2')\n",
    "    plt.title(f'{feature} by Cluster')\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9253c7-6899-44c0-a1d4-03222f17b3a7",
   "metadata": {},
   "source": [
    "Using Bernoulli Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963bc257-b648-4c4c-93cc-01dbcba23dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.special import logsumexp\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your data X is already loaded\n",
    "N, D = X.shape\n",
    "K = 5  # number of clusters\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Data type: {type(X)}\")\n",
    "print(f\"Data range: {X.min().min() if hasattr(X, 'min') else X.min()} to {X.max().max() if hasattr(X, 'max') else X.max()}\")\n",
    "\n",
    "# Convert to numpy if it's a DataFrame\n",
    "if hasattr(X, 'values'):\n",
    "    X_np = X.values\n",
    "else:\n",
    "    X_np = X\n",
    "\n",
    "# METHOD 1: Use sklearn's GaussianMixture (works surprisingly well for binary data)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 1: Sklearn GaussianMixture (surprisingly effective for binary data)\")\n",
    "\n",
    "gmm = GaussianMixture(n_components=K, random_state=42, max_iter=100)\n",
    "gmm.fit(X_np)\n",
    "\n",
    "# Get predictions\n",
    "gmm_labels = gmm.predict(X_np)\n",
    "gmm_probs = gmm.predict_proba(X_np)\n",
    "\n",
    "print(f\"GMM Cluster sizes: {np.bincount(gmm_labels)}\")\n",
    "print(f\"GMM Log-likelihood: {gmm.score(X_np):.2f}\")\n",
    "print(f\"GMM Converged: {gmm.converged_}\")\n",
    "\n",
    "# METHOD 2: Custom EM Algorithm for Bernoulli Mixture\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 2: Custom EM Algorithm for Bernoulli Mixture\")\n",
    "\n",
    "class BernoulliMixture:\n",
    "    def __init__(self, n_components=2, max_iter=100, tol=1e-6, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X):\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        N, D = X.shape\n",
    "        K = self.n_components\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights_ = np.ones(K) / K\n",
    "        self.params_ = np.random.beta(1, 1, size=(K, D))  # Bernoulli parameters\n",
    "        \n",
    "        # Initialize responsibilities\n",
    "        resp = np.random.dirichlet(np.ones(K), N)\n",
    "        \n",
    "        log_likelihood_old = -np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # E-step: compute responsibilities\n",
    "            log_resp = np.zeros((N, K))\n",
    "            \n",
    "            for k in range(K):\n",
    "                # Log likelihood for cluster k\n",
    "                log_like_k = np.sum(X * np.log(self.params_[k] + 1e-10) + \n",
    "                                  (1 - X) * np.log(1 - self.params_[k] + 1e-10), axis=1)\n",
    "                log_resp[:, k] = np.log(self.weights_[k] + 1e-10) + log_like_k\n",
    "            \n",
    "            # Normalize responsibilities\n",
    "            log_resp_norm = logsumexp(log_resp, axis=1, keepdims=True)\n",
    "            log_resp -= log_resp_norm\n",
    "            resp = np.exp(log_resp)\n",
    "            \n",
    "            # M-step: update parameters\n",
    "            resp_sum = resp.sum(axis=0)\n",
    "            self.weights_ = resp_sum / N\n",
    "            \n",
    "            for k in range(K):\n",
    "                self.params_[k] = (resp[:, k:k+1].T @ X) / (resp_sum[k] + 1e-10)\n",
    "                # Clip to avoid numerical issues\n",
    "                self.params_[k] = np.clip(self.params_[k], 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            # Check convergence\n",
    "            log_likelihood = np.sum(log_resp_norm)\n",
    "            if abs(log_likelihood - log_likelihood_old) < self.tol:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "            log_likelihood_old = log_likelihood\n",
    "        \n",
    "        self.log_likelihood_ = log_likelihood\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        N, D = X.shape\n",
    "        K = self.n_components\n",
    "        log_resp = np.zeros((N, K))\n",
    "        \n",
    "        for k in range(K):\n",
    "            log_like_k = np.sum(X * np.log(self.params_[k] + 1e-10) + \n",
    "                              (1 - X) * np.log(1 - self.params_[k] + 1e-10), axis=1)\n",
    "            log_resp[:, k] = np.log(self.weights_[k] + 1e-10) + log_like_k\n",
    "        \n",
    "        log_resp_norm = logsumexp(log_resp, axis=1, keepdims=True)\n",
    "        log_resp -= log_resp_norm\n",
    "        return np.exp(log_resp)\n",
    "\n",
    "# Fit the custom Bernoulli mixture\n",
    "bmm = BernoulliMixture(n_components=K, random_state=42)\n",
    "bmm.fit(X_np)\n",
    "\n",
    "bmm_labels = bmm.predict(X_np)\n",
    "bmm_probs = bmm.predict_proba(X_np)\n",
    "\n",
    "print(f\"BMM Cluster sizes: {np.bincount(bmm_labels)}\")\n",
    "print(f\"BMM Log-likelihood: {bmm.log_likelihood_:.2f}\")\n",
    "print(f\"BMM Weights: {bmm.weights_}\")\n",
    "print(f\"BMM Parameters shape: {bmm.params_.shape}\")\n",
    "\n",
    "# METHOD 3: PyMC for Bayesian approach (if you have PyMC installed)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 3: PyMC Bayesian Bernoulli Mixture (requires PyMC)\")\n",
    "\n",
    "try:\n",
    "    import pymc as pm\n",
    "    import pytensor.tensor as pt\n",
    "    \n",
    "    # Sample a subset for faster computation\n",
    "    sample_size = min(10000, N)\n",
    "    idx = np.random.choice(N, sample_size, replace=False)\n",
    "    X_sample = X_np[idx]\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        weights = pm.Dirichlet('weights', a=np.ones(K))\n",
    "        params = pm.Beta('params', alpha=1, beta=1, shape=(K, D))\n",
    "        \n",
    "        # Cluster assignment\n",
    "        cluster = pm.Categorical('cluster', p=weights, shape=sample_size)\n",
    "        \n",
    "        # Likelihood\n",
    "        obs = pm.Bernoulli('obs', p=params[cluster], observed=X_sample)\n",
    "        \n",
    "        # Inference\n",
    "        trace = pm.sample(1000, tune=500, chains=2, random_seed=42)\n",
    "    \n",
    "    # Get posterior means\n",
    "    pymc_weights = trace.posterior['weights'].mean(dim=['chain', 'draw']).values\n",
    "    pymc_params = trace.posterior['params'].mean(dim=['chain', 'draw']).values\n",
    "    \n",
    "    print(f\"PyMC Weights: {pymc_weights}\")\n",
    "    print(f\"PyMC Parameters shape: {pymc_params.shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PyMC not installed - skipping Bayesian method\")\n",
    "except Exception as e:\n",
    "    print(f\"PyMC method failed: {e}\")\n",
    "\n",
    "# METHOD 4: Simple K-means clustering (baseline)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 4: K-means baseline\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_np)\n",
    "\n",
    "print(f\"K-means Cluster sizes: {np.bincount(kmeans_labels)}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON OF METHODS\")\n",
    "\n",
    "methods = {\n",
    "    'GMM': gmm_labels,\n",
    "    'Bernoulli MM': bmm_labels,\n",
    "    'K-means': kmeans_labels\n",
    "}\n",
    "\n",
    "for name, labels in methods.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Cluster sizes: {np.bincount(labels)}\")\n",
    "    if hasattr(X, 'columns'):\n",
    "        # If X is a DataFrame, show feature means by cluster\n",
    "        cluster_means = pd.DataFrame(X_np).groupby(labels).mean()\n",
    "        print(f\"  Feature means by cluster:\")\n",
    "        print(cluster_means.round(3))\n",
    "\n",
    "# Visualization if reasonable size\n",
    "if D <= 10:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (name, labels) in enumerate(methods.items()):\n",
    "        if i < len(axes):\n",
    "            # Plot first two features colored by cluster\n",
    "            scatter = axes[i].scatter(X_np[:, 0], X_np[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "            axes[i].set_title(f'{name} Clustering')\n",
    "            axes[i].set_xlabel('Feature 0')\n",
    "            axes[i].set_ylabel('Feature 1')\n",
    "            plt.colorbar(scatter, ax=axes[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"1. For simplicity and speed: Use sklearn's GaussianMixture\")\n",
    "print(\"2. For proper Bernoulli modeling: Use the custom BernoulliMixture class\")\n",
    "print(\"3. For Bayesian inference: Use PyMC (if installed)\")\n",
    "print(\"4. For baseline comparison: K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012e85a-b080-4e2a-b36b-d61cf3fc803a",
   "metadata": {},
   "source": [
    "# Mathematical Explanation of Clustering Methods for Binary Data\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Bernoulli Mixture Model (BMM)\n",
    "\n",
    "The Bernoulli Mixture Model assumes each data point \\( \\mathbf{x}_n = (x_{n1}, x_{n2}, \\dots, x_{nD}) \\) is generated by one of \\( K \\) clusters.\n",
    "\n",
    "Each cluster \\( k \\) has:\n",
    "\n",
    "- A **Bernoulli probability vector**:  \n",
    "  $$ \\mathbf{p}_k = (p_{k1}, p_{k2}, \\dots, p_{kD}) $$\n",
    "\n",
    "- A **cluster prior** (mixture weight):  \n",
    "  $$ \\pi_k, \\quad \\text{where} \\quad \\sum_{k=1}^K \\pi_k = 1 $$\n",
    "\n",
    "\n",
    "### Likelihood for a single data point:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}_n \\mid z_n = k) = \\prod_{d=1}^D p_{kd}^{x_{nd}} (1 - p_{kd})^{1 - x_{nd}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### EM Algorithm\n",
    "\n",
    "#### **E-step:** Compute responsibilities (posterior cluster probabilities)\n",
    "\n",
    "$$\n",
    "\\gamma_{nk} = \\frac{\n",
    "\\pi_k \\prod_{d=1}^D p_{kd}^{x_{nd}} (1 - p_{kd})^{1 - x_{nd}}\n",
    "}{\n",
    "\\sum_{j=1}^K \\pi_j \\prod_{d=1}^D p_{jd}^{x_{nd}} (1 - p_{jd})^{1 - x_{nd}}\n",
    "}\n",
    "$$\n",
    "\n",
    "#### **M-step:** Update parameters\n",
    "\n",
    "Update mixture weights:\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{nk}\n",
    "$$\n",
    "\n",
    "Update Bernoulli parameters:\n",
    "\n",
    "$$\n",
    "p_{kd} = \\frac{\\sum_{n=1}^N \\gamma_{nk} x_{nd}}{\\sum_{n=1}^N \\gamma_{nk}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gaussian Mixture Model (GMM)\n",
    "\n",
    "The Gaussian Mixture Model assumes each data point is generated from a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}_n) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\mathbf{x}_n \\mid \\boldsymbol{\\mu}_k, \\Sigma_k)\n",
    "$$\n",
    "\n",
    "- \\( \\boldsymbol{\\mu}_k \\): mean of cluster \\( k \\)\n",
    "- \\( \\Sigma_k \\): covariance matrix of cluster \\( k \\)\n",
    "- \\( \\mathcal{N}(\\cdot) \\): multivariate Gaussian PDF\n",
    "\n",
    "Although GMMs are designed for continuous data, they often work reasonably for binary data due to flexibility in modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. K-means Clustering\n",
    "\n",
    "K-means partitions data into \\( K \\) clusters by minimizing intra-cluster variance:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{C} \\sum_{k=1}^K \\sum_{\\mathbf{x}_n \\in C_k} \\|\\mathbf{x}_n - \\boldsymbol{\\mu}_k\\|^2\n",
    "$$\n",
    "\n",
    "- \\( \\boldsymbol{\\mu}_k \\): centroid of cluster \\( k \\)\n",
    "\n",
    "K-means is simple and fast but lacks probabilistic interpretation and is less suitable for binary data.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Method   | Suitable for Binary? | Probabilistic? | Notes |\n",
    "|----------|----------------------|----------------|-------|\n",
    "| BMM      | ✅ Yes               | ✅ Yes         | Tailored for binary features |\n",
    "| GMM      | ⚠️ Acceptable        | ✅ Yes         | Assumes continuous features |\n",
    "| K-means  | ⚠️ Weak              | ❌ No          | Fast baseline, but not ideal for binary data |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c01bf-43da-4a61-931c-1351fde2f136",
   "metadata": {},
   "source": [
    "Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8010d1e-f599-4abf-9272-6fe8ea79e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.metrics import jaccard_score\n",
    "import seaborn as sns\n",
    "\n",
    "feature_names = [\n",
    "    'abnormal waist circumference', 'abnormal bp', 'abnormal glucose',\n",
    "    'abnormal haemoglobin', 'abnormal bmi'\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "X = table[[\n",
    "    'abnormal waist circumference', 'abnormal bp', 'abnormal glucose',\n",
    "    'abnormal haemoglobin', 'abnormal bmi'\n",
    "]]\n",
    "\n",
    "\n",
    "D = X.shape[1]\n",
    "\n",
    "#Jaccard similarity matrix\n",
    "jaccard_similarity = np.zeros((D, D))\n",
    "for i in range(D):\n",
    "    for j in range(D):\n",
    "        jaccard_similarity[i, j] = jaccard_score(X.iloc[:, i], X.iloc[:, j])\n",
    "\n",
    "\n",
    "jaccard_distance = 1 - jaccard_similarity\n",
    "\n",
    "# Perform hierarchical clustering using average linkage on the distance matrix\n",
    "linkage_matrix = linkage(jaccard_distance, method='average')\n",
    "\n",
    "# no. of clusters\n",
    "num_clusters = 3\n",
    "num_clusters = min(num_clusters, D)  # safety\n",
    "\n",
    "# cluster assignments\n",
    "clusters = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# creating labels with feature name and cluster number for dendrogram leaves\n",
    "labels_with_cluster = [f\"{name} (C{clusters[i]})\" for i, name in enumerate(feature_names)]\n",
    "\n",
    "# dendrogram\n",
    "plt.figure(figsize=(16, 7))\n",
    "dendro = dendrogram(\n",
    "    linkage_matrix,\n",
    "    labels=labels_with_cluster,\n",
    "    leaf_rotation=45,\n",
    "    leaf_font_size=12,\n",
    "    color_threshold=0\n",
    ")\n",
    "\n",
    "#horizontal line for cluster threshold\n",
    "threshold_idx = linkage_matrix.shape[0] - num_clusters\n",
    "threshold = linkage_matrix[threshold_idx, 2]\n",
    "plt.axhline(y=threshold, color='grey', linestyle='--', linewidth=1.5, label='Cluster Threshold')\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram of Binary Features\\n(with Jaccard Distance)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Distance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of jaccard similarity\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    jaccard_similarity,\n",
    "    xticklabels=feature_names,\n",
    "    yticklabels=feature_names,\n",
    "    cmap='viridis',\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    square=True,\n",
    "    cbar_kws={'label': 'Jaccard Similarity'}\n",
    ")\n",
    "plt.title('Feature Similarity Heatmap (Jaccard Index)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cluster assignments and features:\")\n",
    "for c in range(1, num_clusters + 1):\n",
    "    members = [feature_names[i] for i in range(D) if clusters[i] == c]\n",
    "    print(f\"Cluster {c} ({len(members)} features): {', '.join(members)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88390182-1cf3-417e-ac28-84804d2fb75a",
   "metadata": {},
   "source": [
    "# Explanation of Metrics, Methods, and Parameters Used in the Binary Feature Clustering Code\n",
    "\n",
    "## 1. Jaccard Similarity / Distance\n",
    "\n",
    "- **Jaccard Similarity** measures how similar two binary vectors are:\n",
    "\n",
    "  $$\n",
    "  J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "  $$\n",
    "\n",
    "  - Where \\( A \\) and \\( B \\) are sets of positions where the binary features are 1.\n",
    "  - The numerator counts positions where both vectors have 1.\n",
    "  - The denominator counts positions where either vector has 1.\n",
    "\n",
    "- **Range**:\n",
    "  - \\( J = 1 \\): features are identical.\n",
    "  - \\( J = 0 \\): features have no overlap (dissimilar).\n",
    "\n",
    "- **Jaccard Distance** is:\n",
    "\n",
    "  $$\n",
    "  D(A, B) = 1 - J(A, B)\n",
    "  $$\n",
    "\n",
    "- In code, we use `sklearn.metrics.jaccard_score` (with `average=None`) to compute similarity across feature pairs.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Hierarchical (Agglomerative) Clustering\n",
    "\n",
    "- Start with each feature as its own cluster.\n",
    "- At each step, merge the two closest clusters based on a distance measure.\n",
    "\n",
    "- With **average linkage**, the distance between clusters \\( C_i \\) and \\( C_j \\) is:\n",
    "\n",
    "  $$\n",
    "  d(C_i, C_j) = \\frac{1}{|C_i| \\cdot |C_j|} \\sum_{x \\in C_i} \\sum_{y \\in C_j} d(x, y)\n",
    "  $$\n",
    "\n",
    "  - Where \\( d(x, y) \\) is the Jaccard distance between feature vectors \\( x \\) and \\( y \\).\n",
    "  - Average linkage computes the mean of all pairwise distances between members of the two clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d0c27-de3c-4f62-8742-5138237365b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
